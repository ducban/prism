---
# Core Metadata
title: "RICE Prioritization"
title_vi: "∆Øu Ti√™n RICE"
framework_type: "Prioritization"
category: ["Product Management", "Prioritization", "Scoring"]

# Origin & Authority
author: "Intercom Product Team"
organization: "Intercom"
year_developed: "2016"
original_source: "RICE: Simple prioritization for product managers"

# Root Integration
root_phase: ["Phase 2: Prioritize & Decide"]
root_commands: ["/phase2:rice"]
when_to_use: "When you need objective, data-driven prioritization with multiple competing features"

# Difficulty & Time
complexity: "Medium"
estimated_time: "2-4 hours per batch of features"
skill_level: "Intermediate"

# Classification
tags: ["prioritization", "scoring", "data-driven", "objective", "quantitative"]
related_frameworks: ["ICE Scoring", "MoSCoW Prioritization", "Cost-Benefit Analysis"]
conflicts_with: []

# Metadata
version: "1.0"
last_updated: "2025-11-20"
language: "bilingual"
status: "Active"

# Learning Resources
external_resources:
  - type: "Article"
    title: "RICE: Simple prioritization for product managers"
    url: "https://www.intercom.com/blog/rice-simple-prioritization-for-product-managers/"
    author: "Intercom"
    year: "2016"
  - type: "Tool"
    title: "RICE Scoring Calculator"
    url: "https://www.productplan.com/glossary/rice-scoring-model/"
    author: "ProductPlan"
    year: "2024"
  - type: "Guide"
    title: "RICE Prioritization Framework Guide"
    url: "https://www.productplan.com/learn/rice-prioritization/"
    author: "Roadmunk"
    year: "2024"
---

# RICE Prioritization / ∆Øu Ti√™n RICE

## Overview / T·ªïng Quan

**English:**
RICE helps you decide which features to build first by scoring them on 4 factors.

**RICE stands for:**
- **R**each: How many users will this affect?
- **I**mpact: How much will it help each user?
- **C**onfidence: How sure are you about your estimates?
- **E**ffort: How long will it take to build?

**Formula:** Score = (Reach √ó Impact √ó Confidence) / Effort

**Real example - Spotify's "Wrapped" feature:**
- **Reach**: 200 million users (everyone gets a Wrapped)
- **Impact**: 3 (massive - users share it everywhere)
- **Confidence**: 90% (they've done it before)
- **Effort**: 6 person-months
- **Score**: (200,000,000 √ó 3 √ó 0.9) / 6 = 90,000,000

This high score explains why Spotify does Wrapped every year!

**Vietnamese:**
RICE gi√∫p b·∫°n quy·∫øt ƒë·ªãnh t√≠nh nƒÉng n√†o n√™n ƒë∆∞·ª£c x√¢y d·ª±ng tr∆∞·ªõc b·∫±ng c√°ch ch·∫•m ƒëi·ªÉm d·ª±a tr√™n 4 y·∫øu t·ªë.

**RICE l√† vi·∫øt t·∫Øt c·ªßa:**
- **R**each (Ph·∫°m vi): Bao nhi√™u ng∆∞·ªùi d√πng s·∫Ω b·ªã ·∫£nh h∆∞·ªüng?
- **I**mpact (T√°c ƒë·ªông): N√≥ s·∫Ω gi√∫p m·ªói ng∆∞·ªùi d√πng nhi·ªÅu nh∆∞ th·∫ø n√†o?
- **C**onfidence (ƒê·ªô t·ª± tin): B·∫°n ch·∫Øc ch·∫Øn ƒë·∫øn m·ª©c n√†o v·ªÅ ∆∞·ªõc t√≠nh c·ªßa m√¨nh?
- **E**ffort (N·ªó l·ª±c): M·∫•t bao l√¢u ƒë·ªÉ x√¢y d·ª±ng?

**C√¥ng th·ª©c:** ƒêi·ªÉm = (Ph·∫°m vi √ó T√°c ƒë·ªông √ó ƒê·ªô t·ª± tin) / N·ªó l·ª±c

**V√≠ d·ª• th·ª±c t·∫ø - T√≠nh nƒÉng "Wrapped" c·ªßa Spotify:**
- **Ph·∫°m vi**: 200 tri·ªáu ng∆∞·ªùi d√πng (m·ªçi ng∆∞·ªùi ƒë·ªÅu nh·∫≠n Wrapped)
- **T√°c ƒë·ªông**: 3 (r·∫•t l·ªõn - ng∆∞·ªùi d√πng chia s·∫ª kh·∫Øp n∆°i)
- **ƒê·ªô t·ª± tin**: 90% (h·ªç ƒë√£ l√†m tr∆∞·ªõc ƒë√¢y)
- **N·ªó l·ª±c**: 6 ng∆∞·ªùi-th√°ng
- **ƒêi·ªÉm**: (200,000,000 √ó 3 √ó 0.9) / 6 = 90,000,000

ƒêi·ªÉm s·ªë cao n√†y gi·∫£i th√≠ch t·∫°i sao Spotify l√†m Wrapped h√†ng nƒÉm!

---

## RICE Scoring Playbook / H∆∞·ªõng D·∫´n Ch·∫•m ƒêi·ªÉm RICE

**English:**

This playbook provides detailed instructions for scoring each RICE factor. Use this when prioritizing features for the Root project.

### Factor 1: Reach (Ph·∫°m vi)

**Definition**: How many users/customers will this feature affect within a specific time period (usually per quarter or per month)?

**How to Score**:
- Use **actual numbers**, not percentages
- Be specific about the time period (e.g., "per quarter", "per month")
- Base it on data when possible

**Scoring Guide**:

| Reach Score | Meaning | Example |
|------------|---------|---------|
| 0-100 | Tiny niche segment | Internal tool for 5 team members |
| 100-1,000 | Small user group | Feature for enterprise customers (50 companies √ó 20 users each) |
| 1,000-10,000 | Moderate segment | Mobile app feature (10% of 100K users) |
| 10,000-100,000 | Large segment | Core feature (50% of 200K users) |
| 100,000+ | Mass market | Platform-wide change (all users) |

**How to Calculate Reach for Root**:

1. **New Capability**: Estimate how many PMs will use this specific capability per quarter
   - Example: `/phase3:prd` command ‚Üí Assume 70% of active users need PRDs ‚Üí Reach = 700 (if 1,000 active users)

2. **Enhancement**: Count existing users who will benefit
   - Example: Improve RICE scoring ‚Üí All users who prioritize ‚Üí Reach = 1,000

3. **Fix/Improvement**: Count affected users
   - Example: Fix broken command ‚Üí Users who tried and failed ‚Üí Reach = 50

**Common Mistakes to Avoid**:
- ‚ùå Don't use percentages (use actual numbers)
- ‚ùå Don't count potential future users (use current user base)
- ‚ùå Don't double-count (if a user benefits multiple times, count them once)

---

### Factor 2: Impact (T√°c ƒë·ªông)

**Definition**: How much will this feature improve the experience for each affected user?

**How to Score**:
- Use the scale: 0.25 (minimal), 0.5 (low), 1 (medium), 2 (high), 3 (massive)
- Choose ONE value per feature
- Consider: Does this solve a critical pain point or just a nice-to-have?

**Scoring Guide**:

| Impact Score | Meaning | User Reaction | Example |
|-------------|---------|---------------|---------|
| **3 - Massive** | Transformative change | "This changes everything!" | Spotify Wrapped (massive viral sharing), Dark mode (highly requested) |
| **2 - High** | Significant improvement | "This is really helpful!" | Slack Threads (solves major pain point), Auto-save feature |
| **1 - Medium** | Noticeable benefit | "That's nice to have" | UI polish, minor convenience feature |
| **0.5 - Low** | Small improvement | "Barely noticed but useful" | Tooltip improvement, color adjustment |
| **0.25 - Minimal** | Tiny enhancement | "Meh, okay I guess" | Edge case fix, obscure setting |

**How to Determine Impact for Root**:

Ask yourself these questions:

1. **Pain Point Severity**:
   - Does this solve a **blocker** (can't do their job)? ‚Üí Impact = 3
   - Does this solve a **major frustration** (can do it, but it's painful)? ‚Üí Impact = 2
   - Does this make something **easier** (already works, just faster)? ‚Üí Impact = 1
   - Is this just **polish** (minor improvement)? ‚Üí Impact = 0.5

2. **Frequency of Use**:
   - Will users hit this **daily**? ‚Üí Higher impact
   - Will users hit this **weekly**? ‚Üí Medium impact
   - Will users hit this **rarely**? ‚Üí Lower impact

3. **Business Impact**:
   - Does this directly affect **revenue** or **retention**? ‚Üí Higher impact
   - Does this improve **efficiency** significantly? ‚Üí Medium-high impact
   - Is this just **nice to have**? ‚Üí Lower impact

**Examples for Root**:

| Feature | Impact Score | Reasoning |
|---------|-------------|-----------|
| Add `/phase3:prd` command (PRD generation) | 3 (Massive) | Solves major blocker - PMs spend 8 hours on PRDs, reduces to 3 hours |
| Add RICE calculation to `/phase2:rice` | 2 (High) | Significantly improves prioritization - automatic calculation saves time and reduces errors |
| Add color coding to status | 1 (Medium) | Nice visual improvement, easier to scan |
| Fix typo in help text | 0.25 (Minimal) | Barely noticed, doesn't affect functionality |

**Common Mistakes to Avoid**:
- ‚ùå Don't score everything as 3 (be honest - most features are 1 or 2)
- ‚ùå Don't let personal preference bias you (use data and user feedback)
- ‚ùå Don't confuse "cool technology" with "user impact" (users care about value, not tech)

---

### Factor 3: Confidence (ƒê·ªô t·ª± tin)

**Definition**: How confident are you in your Reach and Impact estimates? Express as percentage.

**How to Score**:
- Use percentages: 10%, 20%, 30%... up to 100%
- Be honest - it's better to have low confidence than to overestimate
- Based on quality of data/evidence you have

**Scoring Guide**:

| Confidence % | Meaning | Evidence Quality | Example |
|-------------|---------|------------------|---------|
| **100%** | Absolute certainty | Proven with data from similar feature | Re-launching something that worked before (Spotify Wrapped year 2+) |
| **80%** | Very confident | Strong data supporting estimates | A/B test showed positive results, or validated with users |
| **50%** | Moderate confidence | Some data or qualitative feedback | User interviews suggest need, but no hard data |
| **30%** | Low confidence | Mostly assumptions/intuition | "We think users might want this" |
| **10%** | Very uncertain | Pure speculation | "Maybe this could be useful?" |

**How to Determine Confidence for Root**:

Ask yourself: **What evidence do I have?**

1. **You have STRONG evidence (80-100%)**:
   - ‚úÖ Direct user feedback / feature requests
   - ‚úÖ Usage data from similar features
   - ‚úÖ Competitive analysis (competitors have this)
   - ‚úÖ We've done this before successfully

   Example: Adding `/phase1:incident` command
   - Evidence: 15 users explicitly requested it in surveys
   - Similar: `/phase1:idea` is used 200 times/week
   - Confidence: 80%

2. **You have SOME evidence (50-70%)**:
   - ‚úÖ User interviews mentioned the pain point
   - ‚úÖ Industry best practices suggest it
   - ‚úÖ No data but strong logical reasoning

   Example: Adding RICE scoring guide
   - Evidence: 3 team members asked "how do I score Impact?"
   - Logic: If we provide the tool, we should teach how to use it
   - Confidence: 60%

3. **You're mostly GUESSING (10-40%)**:
   - ‚ùå No user requests
   - ‚ùå No similar features to compare to
   - ‚ùå Just an idea you think is cool

   Example: Adding a "feature mood analyzer"
   - Evidence: None
   - Just an idea: "AI could analyze if features sound exciting"
   - Confidence: 20%

**Common Mistakes to Avoid**:
- ‚ùå Don't default to 100% confidence (rarely justified)
- ‚ùå Don't use confidence to game the score (be honest)
- ‚ùå Don't confuse "I really want this" with "I'm confident in the estimates"

---

### Factor 4: Effort (N·ªó l·ª±c)

**Definition**: How much time will this take to build, test, and ship? Measured in person-months.

**How to Score**:
- Use **person-months**: 1 person working for 1 month = 1 person-month
- Include ALL work: design, development, testing, documentation, deployment
- Be realistic - most things take longer than you think

**Scoring Guide**:

| Effort Score | Time | Complexity | Example |
|------------|------|------------|---------|
| **0.5** | 1-2 weeks | Trivial | Fix typo, update text, minor config change |
| **1** | 1 month | Small | Simple command, basic CRUD, small feature |
| **2** | 2 months | Moderate | Feature with multiple components, integration work |
| **4** | 4 months | Large | Complex feature, new architecture, cross-platform |
| **6+** | 6+ months | Very large | Major platform change, new infrastructure |

**How to Calculate Effort for Root**:

Use this formula:
```
Effort = (Design + Development + Testing + Documentation + Deployment) / Team Size
```

**Example Calculation**:

Feature: Add `/phase3:prd` PRD generation command

1. **Design** (0.5 weeks):
   - Define PRD template structure
   - Design prompt for AI

2. **Development** (2 weeks):
   - Create command files for 5 platforms
   - Implement PRD generation logic
   - Add YAML frontmatter handling

3. **Testing** (1 week):
   - Test on all 5 platforms
   - Fix bugs
   - Validate output quality

4. **Documentation** (0.5 weeks):
   - Add usage examples
   - Update WORKFLOW.md

5. **Deployment** (0.5 weeks):
   - Deploy to all platforms
   - Monitor for issues

**Total**: 4.5 weeks = ~1 month = **Effort: 1**

**Adjustment Factors**:

Multiply effort if:
- üî¥ High uncertainty / new technology ‚Üí 1.5x
- üî¥ Requires coordination across teams ‚Üí 1.3x
- üî¥ High risk / sensitive area ‚Üí 1.5x
- üü¢ Very similar to existing work ‚Üí 0.8x
- üü¢ Template/pattern already exists ‚Üí 0.7x

**Common Mistakes to Avoid**:
- ‚ùå Don't forget testing and documentation (often 30-40% of total effort)
- ‚ùå Don't use "ideal case" estimates (use realistic timelines)
- ‚ùå Don't forget deployment and bug fixes post-launch
- ‚ùå Don't count parallel work as additive (2 people for 1 month = 1 person-month if they can work fully in parallel)

---

## Putting It All Together / T·ªïng H·ª£p

### Final RICE Score Formula

```
RICE Score = (Reach √ó Impact √ó Confidence) / Effort
```

### Complete Example: Adding Dark Mode to Root

**1. Reach**: 1,000 users per quarter (assume 70% of 1,500 active users will use it)

**2. Impact**: 2 (High)
   - Solves major pain point for users working at night
   - Requested by 45 users in surveys
   - Reduces eye strain (measurable benefit)

**3. Confidence**: 80%
   - Strong evidence: Direct user requests
   - Similar features in other tools are highly adopted
   - Industry standard (most apps have dark mode)

**4. Effort**: 3 person-months
   - Design: 1 week (create dark theme palette)
   - Development: 8 weeks (implement across all platforms, test combinations)
   - Testing: 2 weeks (accessibility testing, edge cases)
   - Documentation: 1 week

**Calculation**:
```
RICE = (1,000 √ó 2 √ó 0.8) / 3
     = 1,600 / 3
     = 533.33
```

**Interpretation**: Score of 533 is HIGH - should prioritize this feature!

---

## When to Use RICE vs Other Frameworks

### Use RICE When:
- ‚úÖ You have some data about reach and impact
- ‚úÖ You're comparing multiple features against each other
- ‚úÖ You need an objective scoring system
- ‚úÖ Your team tends to disagree on priorities
- ‚úÖ You want to balance quick wins with strategic bets

### Use ICE Instead When:
- üîÑ You need faster, simpler scoring
- üîÑ You don't have reach data
- üîÑ You're doing quick prioritization in a meeting

### Use MoSCoW Instead When:
- üîÑ You need to communicate to stakeholders simply
- üîÑ You're working with fixed deadlines/budgets
- üîÑ The decision is more about dependencies than value

### Use Cost-Benefit Analysis Instead When:
- üîÑ You need to justify ROI to executives
- üîÑ You're evaluating very expensive initiatives
- üîÑ Revenue impact is the primary concern

---

## Common Mistakes & How to Avoid Them

### Mistake 1: Inflating Scores to Get Your Feature Prioritized
**Problem**: Teams game the system by inflating impact or reach

**Solution**:
- Require evidence for all scores
- Have a second person validate scoring
- Track actual results vs. predicted scores to build calibration

### Mistake 2: Scoring in a Vacuum
**Problem**: Each PM scores their own features without coordination

**Solution**:
- Score features together as a team
- Use the same benchmarks across all features
- Create a "reference feature" everyone knows

### Mistake 3: Forgetting to Update Scores
**Problem**: You score features once and never revisit

**Solution**:
- Re-score quarterly or when new data emerges
- Update confidence as you learn more
- Archive features that are no longer relevant

### Mistake 4: Over-Precision
**Problem**: Spending hours debating if reach is 1,247 or 1,253

**Solution**:
- Use round numbers (1,000 not 1,247)
- Remember: RICE is a prioritization tool, not a scientific formula
- Speed matters more than perfect accuracy

### Mistake 5: Ignoring Strategic Importance
**Problem**: Low-scoring features that are strategically critical get deprioritized

**Solution**:
- Add a "strategic multiplier" for key initiatives
- Separate "strategic bets" from "incremental improvements"
- Be explicit when overriding RICE for strategy

---

## Related Frameworks

### ICE Scoring
**Relationship**: Simpler version of RICE
- Uses 1-10 scales instead of actual numbers
- Faster but less precise
- Good for brainstorming sessions

**When to use ICE**: Quick prioritization, no data available

### MoSCoW Prioritization
**Relationship**: Categorical vs. numerical
- Groups features into Must/Should/Could/Won't
- Less granular than RICE
- Better for stakeholder communication

**When to use MoSCoW**: Fixed scope projects, communicating to non-technical stakeholders

### Cost-Benefit Analysis
**Relationship**: More financial focus
- Emphasizes revenue impact
- Requires more detailed cost modeling
- Better for business cases

**When to use CBA**: Large investments, need executive buy-in

### Kano Model
**Relationship**: Complements RICE
- Classifies features by user satisfaction impact
- Can inform your Impact score in RICE
- Better for understanding feature types

**When to use Kano**: Understanding what delights users vs. what's expected

---

## Learning Resources

### Official Resources
- [RICE: Simple prioritization for product managers](https://www.intercom.com/blog/rice-simple-prioritization-for-product-managers/) - Original article by Intercom (2016)
- [RICE Scoring Calculator](https://www.productplan.com/glossary/rice-scoring-model/) - ProductPlan interactive tool

### Additional Reading
- [How to Use RICE Scoring](https://roadmunk.com/guides/rice-score-prioritization/) - Roadmunk guide
- [RICE Prioritization: The Ultimate Guide](https://www.productboard.com/glossary/rice-scoring/) - Productboard comprehensive guide
- [When RICE Prioritization Fails](https://www.reforge.com/blog/product-prioritization) - Reforge critical analysis

### Templates & Tools
- [RICE Scoring Spreadsheet Template](https://docs.google.com/spreadsheets/d/1VXvwZqLtLwMbBmxhKqgKZvtRLJGHQ5g_JxLPL9LJQiM/edit) - Google Sheets template
- [RICE Score Calculator](https://www.prioritizr.com/rice-calculator) - Online calculator
- [Root `/phase2:rice` command](#) - Built-in Root command

### Case Studies
- **Spotify Wrapped**: How RICE justified annual investment
- **Slack Threads**: Prioritizing collaboration features
- **Notion Database Views**: Balancing power-user vs. casual features

---

## Quick Reference Card

### The Formula
```
RICE = (Reach √ó Impact √ó Confidence) / Effort
```

### Scoring Quick Guide

| Factor | Range | Key Question |
|--------|-------|--------------|
| **Reach** | Actual number | How many users per quarter? |
| **Impact** | 0.25, 0.5, 1, 2, 3 | How much does it help each user? |
| **Confidence** | 10%-100% | How sure are you? |
| **Effort** | Person-months | How long to build? |

### Interpreting Scores

| RICE Score | Priority | Action |
|-----------|----------|--------|
| **500+** | Very High | Do this now |
| **100-500** | High | Schedule this quarter |
| **50-100** | Medium | Consider for backlog |
| **10-50** | Low | Nice to have |
| **<10** | Very Low | Probably skip |

### Common Pitfalls Checklist
- [ ] Did you use actual numbers for Reach (not percentages)?
- [ ] Did you choose only ONE Impact value (not a range)?
- [ ] Did you express Confidence as a decimal (80% = 0.8)?
- [ ] Did you include testing & documentation in Effort?
- [ ] Did you score relative to other features (not in isolation)?
- [ ] Did you document your assumptions and evidence?

---

## Vietnamese Summary / T√≥m T·∫Øt Ti·∫øng Vi·ªát

### C√¥ng Th·ª©c RICE
```
ƒêi·ªÉm RICE = (Ph·∫°m vi √ó T√°c ƒë·ªông √ó ƒê·ªô t·ª± tin) / N·ªó l·ª±c
```

### H∆∞·ªõng D·∫´n Ch·∫•m ƒêi·ªÉm Nhanh

| Y·∫øu T·ªë | Kho·∫£ng Gi√° Tr·ªã | C√¢u H·ªèi Ch√≠nh |
|--------|----------------|---------------|
| **Ph·∫°m vi** | S·ªë th·ª±c t·∫ø | Bao nhi√™u ng∆∞·ªùi d√πng m·ªói qu√Ω? |
| **T√°c ƒë·ªông** | 0.25, 0.5, 1, 2, 3 | Gi√∫p m·ªói ng∆∞·ªùi d√πng nhi·ªÅu nh∆∞ th·∫ø n√†o? |
| **ƒê·ªô t·ª± tin** | 10%-100% | B·∫°n ch·∫Øc ch·∫Øn ƒë·∫øn m·ª©c n√†o? |
| **N·ªó l·ª±c** | Ng∆∞·ªùi-th√°ng | M·∫•t bao l√¢u ƒë·ªÉ x√¢y d·ª±ng? |

### Khi N√†o S·ª≠ D·ª•ng RICE
- ‚úÖ Khi b·∫°n c√≥ d·ªØ li·ªáu v·ªÅ ph·∫°m vi v√† t√°c ƒë·ªông
- ‚úÖ Khi so s√°nh nhi·ªÅu t√≠nh nƒÉng v·ªõi nhau
- ‚úÖ Khi c·∫ßn h·ªá th·ªëng ch·∫•m ƒëi·ªÉm kh√°ch quan
- ‚úÖ Khi nh√≥m th∆∞·ªùng b·∫•t ƒë·ªìng v·ªÅ ∆∞u ti√™n
- ‚úÖ Khi mu·ªën c√¢n b·∫±ng gi·ªØa chi·∫øn th·∫Øng nhanh v√† ƒë·∫∑t c∆∞·ª£c chi·∫øn l∆∞·ª£c

### Sai L·∫ßm Th∆∞·ªùng G·∫∑p
1. **Th·ªïi ph·ªìng ƒëi·ªÉm s·ªë**: Y√™u c·∫ßu b·∫±ng ch·ª©ng cho t·∫•t c·∫£ ƒëi·ªÉm
2. **Ch·∫•m ƒëi·ªÉm ri√™ng l·∫ª**: Ch·∫•m ƒëi·ªÉm c√πng nhau nh∆∞ m·ªôt nh√≥m
3. **Kh√¥ng c·∫≠p nh·∫≠t**: Ch·∫•m l·∫°i h√†ng qu√Ω ho·∫∑c khi c√≥ d·ªØ li·ªáu m·ªõi
4. **Qu√° ch√≠nh x√°c**: S·ª≠ d·ª•ng s·ªë tr√≤n, t·ªëc ƒë·ªô quan tr·ªçng h∆°n ƒë·ªô ch√≠nh x√°c ho√†n h·∫£o
5. **B·ªè qua t·∫ßm quan tr·ªçng chi·∫øn l∆∞·ª£c**: Th√™m "h·ªá s·ªë chi·∫øn l∆∞·ª£c" cho s√°ng ki·∫øn ch√≠nh

---

**Last Updated**: 2025-11-20
**Version**: 1.0
**Root Command**: `/phase2:rice`
**Next Review**: 2025-12-20

---

## Keywords for Further Research

<!-- TODO: Add 5-7 keywords/concepts from this framework for user self-research -->
- [Framework-specific keyword 1]
- [Framework-specific keyword 2]
- [Framework-specific keyword 3]
